# Qwen vLLM

## Online
https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning.html
1.  **Deploy**
    ```bash
    vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \
    --reasoning-parser deepseek_r1 \ # vLLM supports parsing the thinking content from the model generation into structured messages
    --dtype auto \
    --api-key token-abc123
    ```
    The response message will have a field named reasoning_content in addition to content, containing the thinking content generated by the model.
1.  **Run inference**
    ```bash
    uv run client.py
    ```

# Qwen vLLM

## Online
https://qwen.readthedocs.io/en/latest/deployment/vllm.html
1.  **Deploy**
    ```bash
    vllm serve Qwen/Qwen3-8B \
    --dtype auto \
    --api-key token-abc123
    ```
    Parsing Thinking Content
    vLLM supports parsing the thinking content from the model generation into structured messages:
    ```bash
    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1
    ```
    Since vLLM 0.9.0, one can also use
    ```bash
    vllm serve Qwen/Qwen3-8B --reasoning-parser qwen3
    ```
    The response message will have a field named reasoning_content in addition to content, containing the thinking content generated by the model.
1.  **Run inference**
    ```bash
    uv run client.py
    ```
